ㅇㅇ
%학위논문은 대규모 언어모델(LLM)의 Fine-tuning 과정에서 발생하는 하드웨어 자원 소모 문제를 극복하기 위해, 문장 단위 분류 모델과 관계 인식 파인튜닝(relation-aware fine-tuning)이라 명명한 새로운 학습 전략을 제안하며, 적용 도메인으로는 fake news detection을 선정하였다. 본 연구는 두 단계로 수행되었다. 첫 번째 단계에서는 COVID-19 가짜뉴스에 특화된 한국어 데이터셋을 구축하고, KoCharELECTRA 기반 임베딩과 BiLSTM 분류기를 결합한 문장 단위 탐지 모델을 개발하였다. 기존 [CLS] 토큰 기반 분류 모델과 달리, 제안된 모델은 BiLSTM 구조를 도입하여 음절 수준의 맥락 정보를 효과적으로 반영한다. 특히 [CLS] 토큰을 BiLSTM의 은닉 상태와 셀 상태 초기값으로 활용함으로써, 문장의 의미적 흐름을 정교하게 모델링할 수 있도록 설계하였다. 또한 대규모 범용 가짜뉴스 코퍼스를 사전학습한 후, COVID-19 특화 데이터로 추가 파인튜닝을 수행하여 도메인 적응성을 강화하였다. 두 번째 단계에서는 기존 모델 편집 기법의 한계를 극복하기 위해 관계 인식 파인튜닝 프레임워크를 제안하였다. 본 프레임워크는 RAG의 FAISS 기반 의미 검색, Cross Encoder 기반 재랭킹, 자연어 추론(NLI) 기반 관계 분류를 결합하여, 쿼리와 검색된 문장 간의 논리적 관계(함의, 모순, 중립)에 따라 선택적으로 파인튜닝을 적용한다. 실험 결과, 제안된 방법은 기존 파인튜닝과 유사한 정확도를 유지하면서도 locality, 편집 안정성, 일반성, 지식 일반화 측면에서 크게 향상된 성능을 보였다. 특히 인코더 전체를 조정하지 않고 인코더 내 MLP 계층만을 갱신하도록 제한한 결과, 성능 저하 없이 학습 효율성을 크게 개선할 수 있었으며, 이는 자원 제약 환경에서의 실용성을 입증한다. 결론적으로, 본 논문은 데이터 중심의 문장 분류에서 NLI 기반 지식 편집으로 전환하는 새로운 학습 패러다임을 제시한다. 제안된 방법은 가짜뉴스 탐지 성능을 고도화할 뿐 아니라, 지속적인 지식 정제를 통해 신뢰성과 적응성을 갖춘 AI 시스템 구축의 견고한 기반을 제공한다.
